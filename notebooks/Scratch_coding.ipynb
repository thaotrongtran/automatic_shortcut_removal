{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baa69cd",
   "metadata": {
    "id": "2baa69cd"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1721eee3",
   "metadata": {
    "id": "1721eee3"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torchvision.models.resnet import BasicBlock,Bottleneck\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259a31e2",
   "metadata": {
    "id": "259a31e2"
   },
   "outputs": [],
   "source": [
    "path = Path('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578bb440",
   "metadata": {
    "id": "578bb440"
   },
   "outputs": [],
   "source": [
    "class Unet_ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_input = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=64, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                      nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                      nn.ReLU(inplace=True))\n",
    "        layers = []\n",
    "        downsample = nn.Sequential(\n",
    "          nn.Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
    "          nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "        layers.append(Bottleneck(64,64, downsample=downsample))\n",
    "        for _ in range(0, 4):\n",
    "            layers.append(Bottleneck(256, 64))\n",
    "        self.blocks = nn.Sequential(*layers)\n",
    "        self.conv_end = nn.Sequential( nn.Conv2d(in_channels=256, out_channels=3, kernel_size=1, stride=1, padding=0),\n",
    "                                  nn.ReLU(inplace=True))\n",
    "        #Reference source code for initialization of Batch Norm and Conv2d https://pytorch.org/vision/0.8/_modules/torchvision/models/resnet.html\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        orig = x\n",
    "        x = self.conv_input(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.conv_end(x)\n",
    "        x = orig + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe9140f",
   "metadata": {
    "id": "cbe9140f"
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img/2 + 0.5\n",
    "    img_np = img.numpy()\n",
    "    plt.imshow(np.transpose(img_np, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd7a7c4",
   "metadata": {
    "id": "cbd7a7c4"
   },
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a18b125",
   "metadata": {
    "id": "6a18b125"
   },
   "outputs": [],
   "source": [
    "def add_arrow(img):\n",
    "    start = 2\n",
    "    for i in range(start,start+7):\n",
    "        img[:,i,start+6] = -1\n",
    "    for i in range(start+5,start+8):\n",
    "        img[:,start+1,i] = -1\n",
    "    for i in range(start+4,start+9):\n",
    "        img[:,start+2,i] = -1\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87949306",
   "metadata": {
    "id": "87949306"
   },
   "outputs": [],
   "source": [
    "class arrowedCIFAR(Dataset):\n",
    "    \"\"\"Make CIFAR with arrow\"\"\"\n",
    "\n",
    "    def __init__(self, train=True, clean_data = False):\n",
    "        transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        self.train = train\n",
    "        if self.train:\n",
    "            self.cifar = torchvision.datasets.CIFAR10(root = path/'data', download = True, transform=transform, train = True)\n",
    "        else:\n",
    "            self.cifar = torchvision.datasets.CIFAR10(root = path/'data', download = True, transform=transform, train = False)\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        indices = np.random.randint(low = 0, high = len(self.cifar), size=256*20)\n",
    "#         for i in tqdm(range(len(self.cifar))):\n",
    "        for i in tqdm(indices):\n",
    "            img, orig_label = self.cifar.__getitem__(i)\n",
    "            if not clean_data:\n",
    "                img = add_arrow(img)\n",
    "            self.data.append(img) #Only care about the rotation\n",
    "            self.labels.append(0)\n",
    "            for k, angle in enumerate([90, 180, 270]):\n",
    "                img = self.cifar.__getitem__(i)[0]\n",
    "                if not clean_data:\n",
    "                    img = add_arrow(img)\n",
    "                #img = add_arrow(img)\n",
    "                self.data.append(TF.rotate(img, angle))\n",
    "                self.labels.append(k+1) #Add the rest of labels\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b1b575",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71b1b575",
    "outputId": "49228918-c616-4d77-9d0b-b2ca31d72680"
   },
   "outputs": [],
   "source": [
    "trainset = arrowedCIFAR(train=True, clean_data = True)\n",
    "testset = arrowedCIFAR(train=False, clean_data = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quL5NpdrOeuj",
   "metadata": {
    "id": "quL5NpdrOeuj"
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e5de4b",
   "metadata": {
    "id": "39e5de4b"
   },
   "outputs": [],
   "source": [
    "def recon_loss(raw_inputs, lens_output):\n",
    "    loss = nn.MSELoss(reduction = 'sum')\n",
    "    return loss(raw_inputs,lens_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b55955",
   "metadata": {
    "id": "26b55955"
   },
   "outputs": [],
   "source": [
    "def lens_loss(raw_inputs, lens_output, lambda_term, ssl_loss = None, min_probs = None, final_outputs = None):\n",
    "    #Adversarial loss: two types\n",
    "    if ssl_loss:\n",
    "        total_loss = -ssl_loss + lambda_term*recon_loss(raw_inputs, lens_output)\n",
    "    else: \n",
    "        celoss = nn.CrossEntropyLoss(reduction='mean')\n",
    "        adv_loss = celoss(final_outputs,min_probs)\n",
    "        total_loss = adv_loss + lambda_term*recon_loss(raw_inputs, lens_output)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695cbc93",
   "metadata": {
    "id": "695cbc93"
   },
   "outputs": [],
   "source": [
    "class Resnet_FC4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #Feature extraction\n",
    "        res = models.resnet50()\n",
    "        res.fc = torch.nn.Linear(in_features=2048, out_features=4, bias=True)\n",
    "        self.res = res\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.res(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p1lx4pbhKOwK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p1lx4pbhKOwK",
    "outputId": "4aba29d8-e7f2-4bb4-e148-b5dd4d4e2aa2"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac194b72",
   "metadata": {
    "id": "ac194b72"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "sm = nn.Softmax(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae462f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aaf869",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d76fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_terms = [1e-10,1e-10/16,1e-10/32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b15035",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,k in enumerate(lambda_terms):\n",
    "    print(i,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23160929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(lambda_term,ver,epochs):\n",
    "    output_dir = Path(path/f'checkpoints/{ver}')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    f = open(f\"checkpoints/{ver}/logging_{lambda_term}.txt\", \"a\")    \n",
    "    model1 = Unet_ResNet()\n",
    "    model1.to(device)\n",
    "    model2 = Resnet_FC4()\n",
    "    model2.to(device)\n",
    "    #Hyper parameter tuning\n",
    "    lr = 0.01\n",
    "    lens_usage = True\n",
    "    if lens_usage:\n",
    "        optim1 = optim.Adam(model1.parameters(), lr=lr)\n",
    "    optim2 = optim.Adam(model2.parameters(), lr=lr)\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        ssl_losses = 0.0\n",
    "        lens_losses = 0.0\n",
    "        for i, (inputs, labels) in enumerate(trainloader):\n",
    "            #Zero gradients out\n",
    "            if lens_usage:\n",
    "                optim1.zero_grad()\n",
    "            optim2.zero_grad()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            if lens_usage:\n",
    "                lens_output = model1(inputs)\n",
    "                lens_out_detach = lens_output.detach()\n",
    "                lens_out_detach.requires_grad_(True) \n",
    "                outputs = model2(lens_out_detach)\n",
    "                #For type 2 of Adversarial loss\n",
    "                min_probs = torch.argmin(sm(outputs),dim=1)\n",
    "                ssl_loss = criterion(outputs, labels)\n",
    "                #Uncomment if run full adversarial loss\n",
    "    #             l_loss = lens_loss(inputs, lens_out_detach, lambda_term = lambda_term, ssl_loss = ssl_loss)\n",
    "                l_loss = lens_loss(inputs, lens_out_detach, lambda_term = lambda_term, min_probs = min_probs, final_outputs = outputs)\n",
    "                l_loss.backward(retain_graph=True)\n",
    "                lens_output.backward(lens_out_detach.grad) #Let the grad of l_loss go thru\n",
    "                optim2.zero_grad() #Clear out l_loss grad from model2\n",
    "                ssl_loss.backward()\n",
    "            else:\n",
    "                outputs = model2(inputs)\n",
    "                ssl_loss = criterion(outputs, labels)\n",
    "                ssl_loss.backward()\n",
    "            #Update step\n",
    "            if lens_usage:\n",
    "                optim1.step()\n",
    "                lens_losses += l_loss.item()\n",
    "            optim2.step()\n",
    "            ssl_losses += ssl_loss.item()\n",
    "            if i>0 and i % 10 == 0 and epoch % 2 == 0: \n",
    "                print(f'[{epoch}, batch {i}] ssl_loss: {ssl_losses / i:.3f} lens_loss: {lens_losses / i:.3f}')\n",
    "                f.write(f'[{epoch}, batch {i}] ssl_loss: {ssl_losses / i:.3f} lens_loss: {lens_losses / i:.3f}\\n')\n",
    "    #Evaluation\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model2.eval()\n",
    "    model1.eval()#SETTING EVAL MODE\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            if lens_usage:\n",
    "                outputs = model2(model1(images.to(device)))\n",
    "            else:\n",
    "                outputs = model2(images.to(device))\n",
    "            predicted = torch.argmax(sm(outputs), dim = 1).cpu()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy on test images: {100 * correct // total} %')\n",
    "    f.write(f'Accuracy on test images: {100 * correct // total} %\\n')\n",
    "    \n",
    "    #Evaluation on trainloader\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model2.eval()\n",
    "    model1.eval()#SETTING EVAL MODE\n",
    "    with torch.no_grad():\n",
    "        for data in trainloader:\n",
    "            images, labels = data\n",
    "            if lens_usage:\n",
    "                outputs = model2(model1(images.to(device)))\n",
    "            else:\n",
    "                outputs = model2(images.to(device))\n",
    "            predicted = torch.argmax(sm(outputs), dim = 1).cpu()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy on train images: {100 * correct // total} %')\n",
    "    f.write(f'Accuracy on train images: {100 * correct // total} %\\n')\n",
    "    \n",
    "    torch.save(model1, path/f'checkpoints/{ver}/lens_{lambda_term}.pth')\n",
    "    torch.save(model2, path/f'checkpoints/{ver}/extractor_{lambda_term}.pth')\n",
    "    print('========')\n",
    "    f.write('========')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3c2265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper param tuning\n",
    "ver = '006'\n",
    "epochs = 30\n",
    "for term in lambda_terms:\n",
    "    print('Training for', term)\n",
    "    train_loop(term,ver,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea63a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop(lens_usage, model2, testloader, device, model1 = None):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model2.eval()\n",
    "    model1.eval()  # SETTING EVAL MODE\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            if lens_usage:\n",
    "                outputs = model2(model1(images.to(device)))\n",
    "            else:\n",
    "                outputs = model2(images.to(device))\n",
    "            predicted = torch.argmax(sm(outputs), dim=1).cpu()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy on test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0d5960",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '005_30_2e-10'\n",
    "lens_usage = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f586bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if lens_usage:\n",
    "    model1 = torch.load(f'lens_{model_name}.pth')\n",
    "    model1.to(device)\n",
    "else:\n",
    "    model1 = None\n",
    "model2 = torch.load(f'extractor_{model_name}.pth')\n",
    "model2.to(device)\n",
    "eval_loop(lens_usage, model2, testloader, device, model1 = model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2c060b",
   "metadata": {},
   "source": [
    "### Old training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o2q-YkOZXOz8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o2q-YkOZXOz8",
    "outputId": "2ac80f47-aef7-42a6-90b0-e66dc940ce56"
   },
   "outputs": [],
   "source": [
    "model1 = Unet_ResNet()\n",
    "model1.to(device)\n",
    "model2 = Resnet_FC4()\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f4b21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameter tuning\n",
    "lr = 0.01\n",
    "lens_usage = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PohaciUjBZoy",
   "metadata": {
    "id": "PohaciUjBZoy"
   },
   "outputs": [],
   "source": [
    "if lens_usage:\n",
    "    optim1 = optim.Adam(model1.parameters(), lr=lr, betas=(0.1, 0.001), eps=1e-07)\n",
    "optim2 = optim.Adam(model2.parameters(), lr=lr, betas=(0.1, 0.001), eps=1e-07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5552af",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ee6706",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "03ee6706",
    "outputId": "0d048cb9-08ba-4703-fef0-94210a804d68"
   },
   "outputs": [],
   "source": [
    "model1.train()\n",
    "model2.train()\n",
    "for epoch in range(epochs):\n",
    "    ssl_losses = 0.0\n",
    "    lens_losses = 0.0\n",
    "    for i, (inputs, labels) in enumerate(trainloader):\n",
    "        #Zero gradients out\n",
    "        if lens_usage:\n",
    "            optim1.zero_grad()\n",
    "        optim2.zero_grad()\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        if lens_usage:\n",
    "            lens_output = model1(inputs)\n",
    "            lens_out_detach = lens_output.detach()\n",
    "            lens_out_detach.requires_grad_(True) \n",
    "            outputs = model2(lens_out_detach)\n",
    "            #For type 2 of Adversarial loss\n",
    "            min_probs = torch.argmin(sm(outputs),dim=1)\n",
    "            ssl_loss = criterion(outputs, labels)\n",
    "            #Uncomment if run full adversarial loss\n",
    "#             l_loss = lens_loss(inputs, lens_out_detach, lambda_term = lambda_term, ssl_loss = ssl_loss)\n",
    "            l_loss = lens_loss(inputs, lens_out_detach, lambda_term = lambda_term, min_probs = min_probs, final_outputs = outputs)\n",
    "            l_loss.backward(retain_graph=True)\n",
    "            lens_output.backward(lens_out_detach.grad) #Let the grad of l_loss go thru\n",
    "            optim2.zero_grad() #Clear out l_loss grad from model2\n",
    "            ssl_loss.backward()\n",
    "        else:\n",
    "            outputs = model2(inputs)\n",
    "            ssl_loss = criterion(outputs, labels)\n",
    "            ssl_loss.backward()\n",
    "        #Update step\n",
    "        if lens_usage:\n",
    "            optim1.step()\n",
    "            lens_losses += l_loss.item()\n",
    "        optim2.step()\n",
    "        ssl_losses += ssl_loss.item()\n",
    "        #print(model2.res.conv1.weight.grad)\n",
    "        #print(model1.conv_input[0].weight.grad)\n",
    "        if i>0 and i % 50 == 0: \n",
    "            print(f'[{epoch}, batch {i}] ssl_loss: {ssl_losses / i:.3f} lens_loss: {lens_losses / i:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f13e7",
   "metadata": {},
   "source": [
    "## Evaluation of pretext task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f40551",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57d823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation\n",
    "correct = 0\n",
    "total = 0\n",
    "model2.eval()\n",
    "model1.eval()#SETTING EVAL MODE\n",
    "with torch.no_grad():\n",
    "    for data in trainloader:\n",
    "        images, labels = data\n",
    "        if lens_usage:\n",
    "            outputs = model2(model1(images.to(device)))\n",
    "        else:\n",
    "            outputs = model2(images.to(device))\n",
    "        predicted = torch.argmax(sm(outputs), dim = 1).cpu()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Accuracy on test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb8ada4",
   "metadata": {},
   "source": [
    "## Visualization inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9172affc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fbd9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8fb337",
   "metadata": {
    "id": "8c8fb337"
   },
   "outputs": [],
   "source": [
    "model2.eval()\n",
    "model1.eval()\n",
    "with torch.no_grad():\n",
    "    if lens_usage:\n",
    "        img_lensed = model1(img.to(device))\n",
    "        pred = model2(img_lensed)\n",
    "    else:\n",
    "        pred = model2(img.to(device))\n",
    "    print(criterion(pred, label.to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99811bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b6ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(sm(pred), dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fbd562",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeing_index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdd2f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(img[seeing_index].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277a5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if lens_usage:\n",
    "    imshow(img_lensed[seeing_index].squeeze().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30170d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if lens_usage:\n",
    "    imshow(img_lensed[seeing_index].squeeze().cpu() - img[seeing_index].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H3XCdG1npmvO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H3XCdG1npmvO",
    "outputId": "54d512c8-1e44-4791-b9be-803d5ca04323"
   },
   "outputs": [],
   "source": [
    "k = nn.MSELoss(reduction = 'mean')\n",
    "k(img[seeing_index],img_lensed[seeing_index].squeeze().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49bbd21",
   "metadata": {},
   "source": [
    "## Saving model for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113ba7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ver = '004' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22f1953",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model2.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecea519",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model1, path/f'lens_{ver}.pth')\n",
    "torch.save(model2, path/f'extractor_{ver}.pth')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "d85758d6"
   ],
   "name": "Scratch coding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}